# dqn_training.py
# DQN para Exploding Kittens usando el entorno de exploding_env.py

import numpy as np
import random
from collections import deque
import csv
from datetime import datetime
import os

import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim

from exploding_env import ExplodingKittensEnv, valid_actions_from_state, action_name_from_state

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Usando dispositivo:", device)


class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
        )

    def forward(self, x):
        return self.net(x)


class ReplayBuffer:
    def __init__(self, capacity=100_000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            np.stack(states),
            np.array(actions),
            np.array(rewards, dtype=np.float32),
            np.stack(next_states),
            np.array(dones, dtype=np.float32),
        )

    def __len__(self):
        return len(self.buffer)


def select_action(q_net, state, epsilon):
    valid_actions = valid_actions_from_state(state)
    if random.random() < epsilon:
        return random.choice(valid_actions)
    with torch.no_grad():
        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
        q_values = q_net(s).cpu().numpy().flatten()
    best_a = max(valid_actions, key=lambda a: q_values[a])
    return int(best_a)


def train_dqn(
    num_episodes=1500,
    batch_size=64,
    gamma=0.99,
    lr=1e-3,
    epsilon_start=1.0,
    epsilon_end=0.05,
    epsilon_decay_episodes=1000,
    target_update_interval=50,
    use_double_dqn=True,
):
    env = ExplodingKittensEnv()
    state_dim = len(env._get_obs())
    action_dim = 9  # ‚Üê 0-8: Draw, Skip, Attack, Defuse(x3), SeeFuture, DrawBottom, Shuffle

    q_net = QNetwork(state_dim, action_dim).to(device)
    target_net = QNetwork(state_dim, action_dim).to(device)
    target_net.load_state_dict(q_net.state_dict())
    target_net.eval()

    optimizer = optim.Adam(q_net.parameters(), lr=lr)
    replay = ReplayBuffer(capacity=100_000)

    epsilon = epsilon_start

    episode_rewards = []
    episode_wins = []
    
    # Early Stopping variables
    best_win_rate = 0.0
    best_model_state = None
    best_episode = 0
    patience = 10
    no_improvement_count = 0
    min_episodes_before_stopping = 800
    min_winrate_for_stopping = 0.85

    for ep in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0.0
        win_flag = 0

        while not done:
            action = select_action(q_net, state, epsilon)
            next_state, reward, done, info = env.step(action)

            replay.push(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            if done and info.get('winner') == 0:
                win_flag = 1

            if len(replay) >= batch_size:
                states_b, actions_b, rewards_b, next_states_b, dones_b = replay.sample(batch_size)

                states_t = torch.tensor(states_b, dtype=torch.float32, device=device)
                actions_t = torch.tensor(actions_b, dtype=torch.int64, device=device).unsqueeze(1)
                rewards_t = torch.tensor(rewards_b, dtype=torch.float32, device=device).unsqueeze(1)
                next_states_t = torch.tensor(next_states_b, dtype=torch.float32, device=device)
                dones_t = torch.tensor(dones_b, dtype=torch.float32, device=device).unsqueeze(1)

                q_values = q_net(states_t).gather(1, actions_t)

                with torch.no_grad():
                    if use_double_dqn:
                        # Double DQN: usar q_net para seleccionar acci√≥n, target_net para evaluar
                        next_actions = q_net(next_states_t).argmax(1, keepdim=True)
                        next_q_values = target_net(next_states_t).gather(1, next_actions)
                    else:
                        # DQN cl√°sico
                        next_q_values = target_net(next_states_t).max(1, keepdim=True)[0]
                    target = rewards_t + gamma * (1 - dones_t) * next_q_values

                loss = nn.functional.mse_loss(q_values, target)

                optimizer.zero_grad()
                loss.backward()
                # Gradient clipping para estabilidad
                torch.nn.utils.clip_grad_norm_(q_net.parameters(), max_norm=10.0)
                optimizer.step()

        # Linear epsilon decay (como V1)
        if epsilon > epsilon_end:
            epsilon -= (epsilon_start - epsilon_end) / epsilon_decay_episodes
            epsilon = max(epsilon_end, epsilon)

        if (ep + 1) % target_update_interval == 0:
            target_net.load_state_dict(q_net.state_dict())

        episode_rewards.append(total_reward)
        episode_wins.append(win_flag)

        if (ep + 1) % 50 == 0:
            win_rate = np.mean(episode_wins[-50:])
            r_mean = np.mean(episode_rewards[-50:])
            print(f"Ep {ep+1}/{num_episodes} | R_media_50={r_mean:.3f} | WinRate_50={win_rate:.3f} | eps={epsilon:.3f}")
            
            # Early Stopping: solo si WR > 80% y despu√©s de min_episodes
            if (ep + 1) >= min_episodes_before_stopping and win_rate >= min_winrate_for_stopping:
                if win_rate > best_win_rate:
                    best_win_rate = win_rate
                    best_episode = ep + 1
                    best_model_state = q_net.state_dict().copy()
                    no_improvement_count = 0
                    print(f"  ‚úÖ Nuevo mejor modelo! WinRate: {best_win_rate:.3f}")
                    # Guardar checkpoint del mejor modelo
                    os.makedirs('output', exist_ok=True)
                    torch.save(best_model_state, 'output/best_model_checkpoint.pth')
                else:
                    no_improvement_count += 1
                    if no_improvement_count >= patience:
                        print(f"\nüõë Early Stopping activado!")
                        print(f"   Mejor WinRate: {best_win_rate:.3f} en episodio {best_episode}")
                        print(f"   Sin mejora durante {patience * 50} episodios")
                        # Restaurar mejor modelo
                        if best_model_state is not None:
                            q_net.load_state_dict(best_model_state)
                            print(f"   Modelo restaurado al episodio {best_episode}")
                        break
            elif (ep + 1) >= min_episodes_before_stopping:
                # Guardar mejor modelo incluso si no alcanza 80%, pero no hacer early stopping
                if win_rate > best_win_rate:
                    best_win_rate = win_rate
                    best_episode = ep + 1
                    best_model_state = q_net.state_dict().copy()
                    print(f"  ‚úÖ Nuevo mejor modelo! WinRate: {best_win_rate:.3f} (early stopping inactivo hasta 80%)")
                    os.makedirs('output', exist_ok=True)
                    torch.save(best_model_state, 'output/best_model_checkpoint.pth')

    # Si no hubo early stopping, usar el mejor modelo encontrado
    if best_model_state is not None and no_improvement_count < patience:
        print(f"\nüìä Entrenamiento completo. Restaurando mejor modelo (ep {best_episode}, WR: {best_win_rate:.3f})")
        q_net.load_state_dict(best_model_state)
    
    print("Entrenamiento terminado.")
    return q_net, episode_rewards, episode_wins


def evaluate_agent(q_net, num_episodes=500):
    """Eval√∫a el agente y retorna estad√≠sticas + log detallado de cada juego"""
    env = ExplodingKittensEnv()
    wins = 0
    rewards = []
    game_logs = []  # Lista de logs detallados

    for game_idx in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0.0
        turn_count = 0
        actions_attempted = []
        actions_executed = []

        while not done:
            valid_actions = valid_actions_from_state(state)
            with torch.no_grad():
                s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
                q_values = q_net(s).cpu().numpy().flatten()
            action = max(valid_actions, key=lambda a: q_values[a])

            before_hand = env.hands[0].copy() if hasattr(env, 'hands') else {}

            state, reward, done, info = env.step(int(action))
            total_reward += reward
            turn_count += 1
            actions_attempted.append(int(action))

            # Detect if action was actually executed by checking hand changes
            executed = False
            if action == 0:
                executed = True
            elif action == 1 and before_hand.get('Skip', 0) > env.hands[0].get('Skip', 0):
                executed = True
            elif action == 2 and before_hand.get('Attack', 0) > env.hands[0].get('Attack', 0):
                executed = True
            elif action == 6 and before_hand.get('SeeFuture', 0) > env.hands[0].get('SeeFuture', 0):
                executed = True
            elif action == 7 and before_hand.get('DrawBottom', 0) > env.hands[0].get('DrawBottom', 0):
                executed = True
            elif action == 8 and before_hand.get('Shuffle', 0) > env.hands[0].get('Shuffle', 0):
                executed = True

            if executed:
                actions_executed.append(int(action))

            if done and info.get('winner') == 0:
                wins += 1

        rewards.append(total_reward)
        
        game_log = {
            'game_id': game_idx + 1,
            'turns': turn_count,
            'total_reward': total_reward,
            'won': 1 if (done and info.get('winner') == 0) else 0,
            'actions_attempted_sequence': ','.join(map(str, actions_attempted)),
            'actions_executed_sequence': ','.join(map(str, actions_executed)),
        }
        game_logs.append(game_log)

    win_rate = wins / num_episodes
    return win_rate, rewards, game_logs


def evaluate_random_agent(num_episodes=500):
    """Eval√∫a agente random y retorna estad√≠sticas + log detallado.

    Similar a evaluate_agent, registra acciones intentadas y ejecutadas.
    """
    env = ExplodingKittensEnv()
    wins = 0
    rewards = []
    game_logs = []

    for game_idx in range(num_episodes):
        state = env.reset()
        done = False
        total_reward = 0.0
        turn_count = 0
        actions_attempted = []
        actions_executed = []

        while not done:
            valid_actions = valid_actions_from_state(state)
            action = int(random.choice(valid_actions))

            before_hand = env.hands[1].copy() if hasattr(env, 'hands') else {}

            state, reward, done, info = env.step(action)
            total_reward += reward
            turn_count += 1
            actions_attempted.append(action)

            # Detect execution for random agent
            # Note: random agent is player 1 in _opponent_turn, but we keep symmetrical logging
            # For simplicity mark draw as executed; for other actions try to detect hand change
            executed = False
            if action == 0:
                executed = True
            elif action == 1 and before_hand.get('Skip', 0) > env.hands[1].get('Skip', 0):
                executed = True
            elif action == 2 and before_hand.get('Attack', 0) > env.hands[1].get('Attack', 0):
                executed = True
            elif action == 6 and before_hand.get('SeeFuture', 0) > env.hands[1].get('SeeFuture', 0):
                executed = True
            elif action == 7 and before_hand.get('DrawBottom', 0) > env.hands[1].get('DrawBottom', 0):
                executed = True
            elif action == 8 and before_hand.get('Shuffle', 0) > env.hands[1].get('Shuffle', 0):
                executed = True

            if executed:
                actions_executed.append(action)

            if done and info.get('winner') == 0:
                wins += 1

        rewards.append(total_reward)
        
        game_log = {
            'game_id': game_idx + 1,
            'turns': turn_count,
            'total_reward': total_reward,
            'won': 1 if (done and info.get('winner') == 0) else 0,
            'actions_attempted_sequence': ','.join(map(str, actions_attempted)),
            'actions_executed_sequence': ','.join(map(str, actions_executed)),
        }
        game_logs.append(game_log)

    win_rate = wins / num_episodes
    return win_rate, rewards, game_logs


def save_validation_logs_to_csv(game_logs, filename='validation_log.csv'):
    """Guarda logs de validaci√≥n en CSV con columnas: game_id, turns, reward, won, actions"""
    os.makedirs('output', exist_ok=True)
    filepath = os.path.join('output', filename)
    with open(filepath, 'w', newline='') as f:
        # Soportar tanto actions_attempted_sequence como actions_executed_sequence
        fieldnames = ['game_id', 'turns', 'total_reward', 'won', 'actions_attempted_sequence', 'actions_executed_sequence']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(game_logs)
    print(f"üìù Log guardado en: {filepath}")


def print_validation_summary(game_logs, agent_name="DQN"):
    """Imprime tabla resumen de estad√≠sticas de validaci√≥n"""
    turns_list = [g['turns'] for g in game_logs]
    rewards_list = [g['total_reward'] for g in game_logs]
    wins = sum(g['won'] for g in game_logs)
    
    # An√°lisis de acciones (usar executed si existe, sino attempted)
    action_counts = {i: 0 for i in range(9)}
    for game in game_logs:
        seq_key = 'actions_executed_sequence' if 'actions_executed_sequence' in game and game['actions_executed_sequence'] else 'actions_attempted_sequence'
        seq = game.get(seq_key, '')
        if seq:
            actions = list(map(int, seq.split(',')))
        else:
            actions = []
        for a in actions:
            action_counts[a] += 1
    
    total_actions = sum(action_counts.values())
    action_names = ["Draw", "Skip", "Attack", "Defuse1", "Defuse2", "Defuse3", "SeeFuture", "DrawBottom", "Shuffle"]
    
    print(f"\n{'='*60}")
    print(f"üìä RESUMEN DE VALIDACI√ìN - {agent_name}")
    print(f"{'='*60}")
    print(f"Juegos totales: {len(game_logs)}")
    print(f"Win Rate: {wins/len(game_logs)*100:.2f}% ({wins}/{len(game_logs)})")
    print(f"Turnos promedio: {np.mean(turns_list):.2f} ¬± {np.std(turns_list):.2f}")
    print(f"Turnos [min, max]: [{min(turns_list)}, {max(turns_list)}]")
    print(f"Reward promedio: {np.mean(rewards_list):.2f} ¬± {np.std(rewards_list):.2f}")
    print(f"\n{'Acci√≥n':<15} {'Total':<10} {'%':<10}")
    print(f"{'-'*35}")
    for i in range(9):
        pct = (action_counts[i] / total_actions * 100) if total_actions > 0 else 0
        print(f"{action_names[i]:<15} {action_counts[i]:<10} {pct:>6.2f}%")
    print(f"{'='*60}\n")


def moving_average(x, w):
    if len(x) < w:
        return np.array(x)
    return np.convolve(x, np.ones(w)/w, mode='valid')


if __name__ == "__main__":
    # Entrenar
    num_episodes = 2000
    q_net, episode_rewards, episode_wins = train_dqn(num_episodes=num_episodes)
    
    # Gr√°ficas de entrenamiento
    window = 50
    ma_rewards = moving_average(episode_rewards, window)
    ma_wins = moving_average(episode_wins, window)

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(ma_rewards)
    plt.title("ENTRENAMIENTO - Recompensa media (ventana=50)")
    plt.xlabel("Episodio")
    plt.ylabel("Recompensa")

    plt.subplot(1, 2, 2)
    plt.plot(ma_wins)
    plt.axhline(y=0.85, color='g', linestyle='--', alpha=0.5, label='Target 85%')
    plt.axhline(y=max(ma_wins), color='r', linestyle='--', alpha=0.5, label=f'Pico {max(ma_wins):.1%}')
    plt.title("ENTRENAMIENTO - Win rate (ventana=50)")
    plt.xlabel("Episodio")
    plt.ylabel("Proporcion victorias")
    plt.legend()
    plt.tight_layout()
    os.makedirs('output', exist_ok=True)
    plt.savefig('output/training_curves_v2.png', dpi=150)
    print("üìä Gr√°ficas de entrenamiento guardadas en output/training_curves_v2.png")
    plt.show()

    # ================================
    # VALIDACI√ìN CON LOGGING DETALLADO
    # ================================
    print("\n" + "="*60)
    print("üß™ FASE DE VALIDACI√ìN")
    print("="*60)
    
    eval_episodes = 400
    
    # Evaluar DQN
    print(f"\nü§ñ Evaluando DQN Agent ({eval_episodes} juegos)...")
    win_rate_dqn, rewards_dqn, game_logs_dqn = evaluate_agent(q_net, num_episodes=eval_episodes)
    
    # Guardar CSV y mostrar resumen
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"validation_dqn_{timestamp}.csv"
    save_validation_logs_to_csv(game_logs_dqn, filename=csv_filename)
    print_validation_summary(game_logs_dqn, agent_name="DQN Agent")
    
    # Evaluar Random
    print(f"\nüé≤ Evaluando Random Agent ({eval_episodes} juegos)...")
    win_rate_rand, rewards_rand, game_logs_rand = evaluate_random_agent(num_episodes=eval_episodes)
    
    csv_filename_rand = f"validation_random_{timestamp}.csv"
    save_validation_logs_to_csv(game_logs_rand, filename=csv_filename_rand)
    print_validation_summary(game_logs_rand, agent_name="Random Agent")
    
    # ================================
    # GR√ÅFICAS DE VALIDACI√ìN
    # ================================
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Comparaci√≥n de Win Rates
    axes[0, 0].bar(['DQN', 'Random'], [win_rate_dqn, win_rate_rand], color=['#2ecc71', '#e74c3c'])
    axes[0, 0].set_ylabel('Win Rate')
    axes[0, 0].set_title('VALIDACION - Win Rate Comparison')
    axes[0, 0].set_ylim([0, 1])
    for i, (agent, wr) in enumerate([('DQN', win_rate_dqn), ('Random', win_rate_rand)]):
        axes[0, 0].text(i, wr + 0.02, f'{wr:.1%}', ha='center', fontweight='bold')
    
    # 2. Distribuci√≥n de Turnos
    axes[0, 1].hist([g['turns'] for g in game_logs_dqn], bins=20, alpha=0.7, label='DQN', color='#3498db')
    axes[0, 1].hist([g['turns'] for g in game_logs_rand], bins=20, alpha=0.7, label='Random', color='#e67e22')
    axes[0, 1].set_xlabel('Turnos por juego')
    axes[0, 1].set_ylabel('Frecuencia')
    axes[0, 1].set_title('VALIDACION - Distribucion de Turnos')
    axes[0, 1].legend()
    
    # 3. Distribuci√≥n de Rewards
    axes[1, 0].hist([g['total_reward'] for g in game_logs_dqn], bins=20, alpha=0.7, label='DQN', color='#9b59b6')
    axes[1, 0].hist([g['total_reward'] for g in game_logs_rand], bins=20, alpha=0.7, label='Random', color='#34495e')
    axes[1, 0].set_xlabel('Total Reward')
    axes[1, 0].set_ylabel('Frecuencia')
    axes[1, 0].set_title('VALIDACION - Distribucion de Rewards')
    axes[1, 0].legend()
    
    # 4. Porcentaje de uso de acciones (DQN)
    action_counts_dqn = {i: 0 for i in range(9)}
    for game in game_logs_dqn:
        # Usar actions_executed_sequence si existe, sino actions_attempted_sequence
        seq = game.get('actions_executed_sequence', game.get('actions_attempted_sequence', ''))
        if seq:
            actions = list(map(int, seq.split(',')))
            for a in actions:
                action_counts_dqn[a] += 1
    
    total_actions_dqn = sum(action_counts_dqn.values())
    action_names = ["Draw", "Skip", "Attack", "Def1", "Def2", "Def3", "SeeFut", "DrwBot", "Shuff"]
    action_pcts = [(action_counts_dqn[i] / total_actions_dqn * 100) if total_actions_dqn > 0 else 0 for i in range(9)]
    
    bars = axes[1, 1].bar(action_names, action_pcts, color='#1abc9c')
    axes[1, 1].set_xlabel('Accion')
    axes[1, 1].set_ylabel('% de uso')
    axes[1, 1].set_title('VALIDACION - Uso de Acciones (DQN)')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # Agregar valores en las barras
    for bar, pct in zip(bars, action_pcts):
        height = bar.get_height()
        if height > 1:  # Solo mostrar si es > 1%
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                          f'{pct:.1f}%', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    validation_plot_filename = f'validation_analysis_{timestamp}.png'
    plt.savefig(os.path.join('output', validation_plot_filename), dpi=150)
    print(f"\nüìä Gr√°ficas de validaci√≥n guardadas en output/{validation_plot_filename}")
    plt.show()

    # Guardar modelo final (ya tiene el mejor modelo cargado gracias a early stopping)
    os.makedirs('output', exist_ok=True)
    torch.save(q_net.state_dict(), "output/dqn_exploding_kittens_v2.pth")
    print("\n‚úÖ Modelo final guardado en output/dqn_exploding_kittens_v2.pth")
    
    # Tambi√©n conservar el checkpoint del mejor modelo
    if os.path.exists('output/best_model_checkpoint.pth'):
        print("‚úÖ Mejor modelo checkpoint disponible en output/best_model_checkpoint.pth")
    
    # ================================
    # GR√ÅFICAS COMPARATIVAS ADICIONALES
    # ================================
    print("\n" + "="*60)
    print("üìä GENERANDO GR√ÅFICAS COMPARATIVAS ADICIONALES")
    print("="*60)
    
    # 1. Comparaci√≥n Win Rate: Training vs Validation
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Training win rate
    axes[0].plot(ma_wins, label='Training', color='#3498db', linewidth=2)
    axes[0].axhline(y=0.85, color='green', linestyle='--', alpha=0.5, label='Target 85%')
    axes[0].axhline(y=max(ma_wins), color='red', linestyle='--', alpha=0.5, label=f'Peak {max(ma_wins):.1%}')
    axes[0].set_xlabel('Episodio', fontsize=12)
    axes[0].set_ylabel('Win Rate', fontsize=12)
    axes[0].set_title('TRAINING - Win Rate Evolution', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Validation comparison
    validation_wr_dqn = win_rate_dqn * 100
    validation_wr_rand = win_rate_rand * 100
    
    bars = axes[1].bar(['DQN Agent', 'Random Agent'], 
                       [validation_wr_dqn, validation_wr_rand],
                       color=['#2ecc71', '#e74c3c'], 
                       alpha=0.8, 
                       edgecolor='black',
                       linewidth=2)
    
    # Agregar valores encima de las barras
    for bar, val in zip(bars, [validation_wr_dqn, validation_wr_rand]):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{val:.1f}%', ha='center', va='bottom', fontsize=14, fontweight='bold')
    
    axes[1].set_ylabel('Win Rate (%)', fontsize=12)
    axes[1].set_title('VALIDATION - Win Rate Comparison', fontsize=14, fontweight='bold')
    axes[1].set_ylim(0, 110)
    axes[1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('output/comparison_training_vs_validation.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Gr√°fica guardada: output/comparison_training_vs_validation.png")
    plt.close()
    
    # 2. An√°lisis de Convergencia: Loss + Win Rate + Epsilon
    fig, axes = plt.subplots(3, 1, figsize=(12, 10))
    
    # Win Rate
    axes[0].plot(ma_wins, color='#2ecc71', linewidth=2)
    axes[0].fill_between(range(len(ma_wins)), ma_wins, alpha=0.3, color='#2ecc71')
    axes[0].axhline(y=0.85, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Target 85%')
    axes[0].set_ylabel('Win Rate', fontsize=12, fontweight='bold')
    axes[0].set_title('CONVERGENCE ANALYSIS', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3)
    axes[0].set_xlim(0, len(ma_wins))
    
    # Reward (si est√° disponible)
    axes[1].plot(ma_rewards, color='#3498db', linewidth=2)
    axes[1].fill_between(range(len(ma_rewards)), ma_rewards, alpha=0.3, color='#3498db')
    axes[1].set_ylabel('Avg Reward', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    axes[1].set_xlim(0, len(ma_rewards))
    
    # Epsilon decay
    epsilon_values = [max(0.05, 1.0 - (i / 1000) * 0.95) for i in range(len(ma_wins))]
    axes[2].plot(epsilon_values, color='#e74c3c', linewidth=2)
    axes[2].fill_between(range(len(epsilon_values)), epsilon_values, alpha=0.3, color='#e74c3c')
    axes[2].set_xlabel('Episodio', fontsize=12, fontweight='bold')
    axes[2].set_ylabel('Epsilon', fontsize=12, fontweight='bold')
    axes[2].grid(True, alpha=0.3)
    axes[2].set_xlim(0, len(epsilon_values))
    
    plt.tight_layout()
    plt.savefig('output/convergence_analysis.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Gr√°fica guardada: output/convergence_analysis.png")
    plt.close()
    
    # 3. An√°lisis de Estrategia: Comparaci√≥n de uso de acciones DQN vs Random
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Uso de acciones DQN
    action_counts_dqn = {i: 0 for i in range(9)}
    for game in game_logs_dqn:
        seq = game.get('actions_executed_sequence', game.get('actions_attempted_sequence', ''))
        if seq:
            actions = list(map(int, seq.split(',')))
            for a in actions:
                action_counts_dqn[a] += 1
    
    total_actions_dqn = sum(action_counts_dqn.values())
    action_names = ["Draw", "Skip", "Attack", "Defuse\nTop", "Defuse\nMid", "Defuse\nBot", "See\nFuture", "Draw\nBottom", "Shuffle"]
    action_pcts_dqn = [(action_counts_dqn[i] / total_actions_dqn * 100) if total_actions_dqn > 0 else 0 for i in range(9)]
    
    # Uso de acciones Random
    action_counts_rand = {i: 0 for i in range(9)}
    for game in game_logs_rand:
        seq = game.get('actions_executed_sequence', game.get('actions_attempted_sequence', ''))
        if seq:
            actions = list(map(int, seq.split(',')))
            for a in actions:
                action_counts_rand[a] += 1
    
    total_actions_rand = sum(action_counts_rand.values())
    action_pcts_rand = [(action_counts_rand[i] / total_actions_rand * 100) if total_actions_rand > 0 else 0 for i in range(9)]
    
    # DQN Agent
    bars1 = axes[0].bar(action_names, action_pcts_dqn, color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5)
    axes[0].set_ylabel('% de Uso', fontsize=12, fontweight='bold')
    axes[0].set_title('DQN Agent - Action Distribution', fontsize=14, fontweight='bold')
    axes[0].tick_params(axis='x', rotation=45, labelsize=9)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    for bar, pct in zip(bars1, action_pcts_dqn):
        height = bar.get_height()
        if height > 1:
            axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{pct:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    # Random Agent
    bars2 = axes[1].bar(action_names, action_pcts_rand, color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5)
    axes[1].set_ylabel('% de Uso', fontsize=12, fontweight='bold')
    axes[1].set_title('Random Agent - Action Distribution', fontsize=14, fontweight='bold')
    axes[1].tick_params(axis='x', rotation=45, labelsize=9)
    axes[1].grid(True, alpha=0.3, axis='y')
    
    for bar, pct in zip(bars2, action_pcts_rand):
        height = bar.get_height()
        if height > 1:
            axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{pct:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('output/strategy_comparison_dqn_vs_random.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Gr√°fica guardada: output/strategy_comparison_dqn_vs_random.png")
    plt.close()
    
    # 4. Comparaci√≥n de Performance Metrics
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Win Rate Comparison
    axes[0, 0].bar(['DQN', 'Random'], [validation_wr_dqn, validation_wr_rand], 
                   color=['#2ecc71', '#e74c3c'], alpha=0.8, edgecolor='black', linewidth=2)
    axes[0, 0].set_ylabel('Win Rate (%)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('Win Rate', fontsize=12, fontweight='bold')
    axes[0, 0].set_ylim(0, 110)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    for i, (label, val) in enumerate([('DQN', validation_wr_dqn), ('Random', validation_wr_rand)]):
        axes[0, 0].text(i, val + 2, f'{val:.1f}%', ha='center', fontsize=12, fontweight='bold')
    
    # Avg Game Length
    avg_len_dqn = np.mean([g['turns'] for g in game_logs_dqn])
    avg_len_rand = np.mean([g['turns'] for g in game_logs_rand])
    
    axes[0, 1].bar(['DQN', 'Random'], [avg_len_dqn, avg_len_rand], 
                   color=['#3498db', '#95a5a6'], alpha=0.8, edgecolor='black', linewidth=2)
    axes[0, 1].set_ylabel('Avg Turns', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('Average Game Length', fontsize=12, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    for i, (label, val) in enumerate([('DQN', avg_len_dqn), ('Random', avg_len_rand)]):
        axes[0, 1].text(i, val + 0.5, f'{val:.1f}', ha='center', fontsize=12, fontweight='bold')
    
    # Avg Reward
    avg_reward_dqn = np.mean([g['total_reward'] for g in game_logs_dqn])
    avg_reward_rand = np.mean([g['total_reward'] for g in game_logs_rand])
    
    axes[1, 0].bar(['DQN', 'Random'], [avg_reward_dqn, avg_reward_rand], 
                   color=['#9b59b6', '#34495e'], alpha=0.8, edgecolor='black', linewidth=2)
    axes[1, 0].set_ylabel('Avg Reward', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('Average Total Reward', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    for i, (label, val) in enumerate([('DQN', avg_reward_dqn), ('Random', avg_reward_rand)]):
        axes[1, 0].text(i, val + 0.05, f'{val:.2f}', ha='center', fontsize=12, fontweight='bold')
    
    # Games Won Pie Chart
    wins_count_dqn = sum([g['won'] for g in game_logs_dqn])
    wins_count_rand = sum([g['won'] for g in game_logs_rand])
    
    axes[1, 1].pie([wins_count_dqn, wins_count_rand], 
                   labels=[f'DQN\n{wins_count_dqn} wins', f'Random\n{wins_count_rand} wins'],
                   colors=['#2ecc71', '#e74c3c'], 
                   autopct='%1.1f%%',
                   startangle=90,
                   explode=(0.05, 0),
                   shadow=True,
                   textprops={'fontsize': 11, 'fontweight': 'bold'})
    axes[1, 1].set_title('Victory Distribution', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('output/performance_metrics_comparison.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Gr√°fica guardada: output/performance_metrics_comparison.png")
    plt.close()
    
    print("\nüìä RESUMEN DE GR√ÅFICAS GENERADAS:")
    print("  1. training_curves_v2.png - Curvas de entrenamiento (reward + win rate)")
    print("  2. validation_analysis_[timestamp].png - An√°lisis de validaci√≥n detallado")
    print("  3. comparison_training_vs_validation.png - Comparaci√≥n training vs validation")
    print("  4. convergence_analysis.png - An√°lisis de convergencia (win rate, reward, epsilon)")
    print("  5. strategy_comparison_dqn_vs_random.png - Comparaci√≥n de estrategias")
    print("  6. performance_metrics_comparison.png - M√©tricas de performance comparadas")
    
    print("\n" + "="*60)
    print("üéâ ENTRENAMIENTO Y VALIDACI√ìN COMPLETADOS")
    print("="*60)
