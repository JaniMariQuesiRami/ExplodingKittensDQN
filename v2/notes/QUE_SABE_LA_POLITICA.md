# ğŸ§  Â¿QuÃ© Sabe la PolÃ­tica del DQN?

## ğŸ“‹ Resumen Ejecutivo

La polÃ­tica (red neuronal DQN) **NO sabe directamente** quÃ© cartas tiene en la mano. Solo recibe **11 nÃºmeros** como entrada (el "estado"), y debe inferir quÃ© hacer basÃ¡ndose en esos nÃºmeros.

---

## ğŸ” **ObservaciÃ³n del Estado (11 valores)**

Cuando el agente toma una decisiÃ³n, la red neuronal recibe estos **11 nÃºmeros**:

```python
# Archivo: exploding_env.py, lÃ­neas 187-217
def _get_obs(self):
    deck_size = len(self.deck)
    bombs = self._count_bombs_in_deck()
    h0 = self.hands[0]  # Mano del agente
    h1 = self.hands[1]  # Mano del oponente
    
    bomb_prob = (bombs / deck_size) if deck_size > 0 else 0.0
    opp_cards = h1['Defuse'] + h1['Skip'] + h1['Attack'] + h1['Safe']
    
    obs = np.array([
        deck_size_norm,           # 0. TamaÃ±o del deck (normalizado)
        bombs_norm,               # 1. NÃºmero de bombas (normalizado)
        bomb_prob,                # 2. Probabilidad de bomba (%)
        float(h0['Defuse']),      # 3. CuÃ¡ntos Defuse tengo
        float(h0['Skip']),        # 4. CuÃ¡ntos Skip tengo
        float(h0['Attack']),      # 5. CuÃ¡ntos Attack tengo
        float(self.pending_draws[0]),  # 6. CuÃ¡ntas cartas debo robar
        opp_cards_norm,           # 7. Total de cartas del oponente (normalizado)
        last_opp_skip,            # 8. Â¿Oponente usÃ³ Skip? (1.0 o 0.0)
        last_opp_attack,          # 9. Â¿Oponente usÃ³ Attack? (1.0 o 0.0)
        phase_defuse,             # 10. Â¿Estoy en fase defuse? (1.0 o 0.0)
    ], dtype=np.float32)
```

---

## âŒ **Lo Que NO EstÃ¡ en la ObservaciÃ³n**

| InformaciÃ³n | Â¿EstÃ¡ en el estado? | RazÃ³n |
|-------------|---------------------|-------|
| **SeeFuture** | âŒ NO | No se incluye `h0['SeeFuture']` |
| **DrawBottom** | âŒ NO | No se incluye `h0['DrawBottom']` |
| **Shuffle** | âŒ NO | No se incluye `h0['Shuffle']` |
| **Safe cards** | âŒ NO | No se incluye `h0['Safe']` |
| Top 3 cartas del deck | âŒ NO | SeeFuture lo muestra al humano, pero no estÃ¡ en el estado |

---

## ğŸ¯ **Por QuÃ© la PolÃ­tica Puede Elegir Acciones InvÃ¡lidas**

### **Ejemplo: Elegir SeeFuture sin tener la carta**

1. **Entrada de la red neuronal**:
   ```python
   state = [0.67, 0.15, 0.22, 1.0, 1.0, 0.0, 1.0, 0.20, 0.0, 0.0, 0.0]
   #         ^^^ deck  ^^^ bombs ^^^ Defuse Skip Attack pending_draws...
   # Nota: No dice cuÃ¡ntos SeeFuture tiene
   ```

2. **Salida de la red neuronal** (Q-values para 9 acciones):
   ```python
   Q-values = [0.5, 0.3, 0.4, 0.1, 0.1, 0.1, 0.9, 0.6, 0.2]
   #           ^^^ Draw      ^^^ Skip       ^^^ SeeFuture (mÃ¡s alto!)
   ```

3. **DecisiÃ³n**:
   - La red dice: "Mejor acciÃ³n = 6 (SeeFuture)" porque tiene Q-value = 0.9
   - **Pero el agente no tiene SeeFuture en mano**

4. **EjecuciÃ³n en el entorno**:
   ```python
   # Archivo: exploding_env.py, lÃ­neas 295-300
   elif action == 6 and self.hands[0]['SeeFuture'] > 0:
       self.hands[0]['SeeFuture'] -= 1
       # ... ejecuta SeeFuture
   ```
   - **CondiciÃ³n falla**: `self.hands[0]['SeeFuture']` es 0
   - **No se ejecuta nada**, pero la acciÃ³n **SÃ se registra** en el CSV

---

## ğŸ”„ **Flujo Completo: Intento vs EjecuciÃ³n**

### **Turno tÃ­pico del agente:**

```
1. Red neuronal recibe estado (11 nÃºmeros)
   â””â”€> [deck=0.67, bombs=0.15, ... Defuse=1, Skip=0, Attack=1, ...]
   
2. Red neuronal calcula Q-values para 9 acciones
   â””â”€> [Q0=0.5, Q1=0.3, ... Q6=0.9 (SeeFuture), ...]
   
3. PolÃ­tica elige acciÃ³n con mayor Q-value
   â””â”€> action = 6 (SeeFuture)
   
4. Se registra en CSV: "IntentÃ³ acciÃ³n 6"
   â””â”€> actions_attempted_sequence += "6"
   
5. Entorno verifica si puede ejecutar acciÃ³n 6
   â””â”€> if self.hands[0]['SeeFuture'] > 0:  # FALSE!
   
6. NO se ejecuta, NO se consume carta
   â””â”€> actions_executed_sequence NO incluye "6"
   
7. Entorno continÃºa al siguiente paso (robar cartas obligatorias)
```

---

## ğŸ“Š **Ejemplo Real de Tu CSV**

### **Juego #1** (del CSV anterior):
```
game_id: 1
turns: 3
actions_attempted_sequence: "0,6,7"
actions_executed_sequence: "6,7"  â† Nota: falta el 0
```

**Â¿QuÃ© pasÃ³?**

1. **Turno 1**: Red elige `0` (Draw)
   - Intento registrado: âœ…
   - Â¿Ejecutado? Depende de si habÃ­a pending_draws > 0
   - Si pending_draws ya estaba en 1, el Draw es obligatorio al final del turno
   - Pero la acciÃ³n `0` explÃ­cita puede no consumir nada

2. **Turno 2**: Red elige `6` (SeeFuture)
   - Intento registrado: âœ…
   - Â¿Ejecutado? âœ… (el agente SÃ tenÃ­a SeeFuture)
   - Se consumiÃ³ la carta

3. **Turno 3**: Red elige `7` (DrawBottom)
   - Intento registrado: âœ…
   - Â¿Ejecutado? âœ… (el agente SÃ tenÃ­a DrawBottom)
   - Se robÃ³ del fondo del deck

---

## ğŸ’¡ **Â¿Por QuÃ© el Sistema Funciona AsÃ­?**

### **Ventajas:**
1. **Simplicidad del espacio de acciÃ³n**: La red siempre tiene 9 opciones, no necesita saber cuÃ¡ntas cartas tiene
2. **Aprendizaje por prueba y error**: La red aprende que elegir acciÃ³n 6 sin tener la carta **no sirve para nada** (no cambia el estado)
3. **GeneralizaciÃ³n**: La red debe aprender a inferir quÃ© cartas tiene basÃ¡ndose en el historial

### **Desventajas:**
1. **Acciones desperdiciadas**: La red pierde turnos eligiendo acciones invÃ¡lidas
2. **Espacio de bÃºsqueda mÃ¡s grande**: La red debe aprender 9 acciones Ã— mÃºltiples estados
3. **CSV confuso**: Las secuencias "intentadas" incluyen acciones que no pasaron nada

---

## ğŸ”§ **SoluciÃ³n Implementada: Filtrado de Acciones VÃ¡lidas**

Hay una funciÃ³n `valid_actions_from_state()` que **limita** las opciones:

```python
# Archivo: exploding_env.py, lÃ­neas 454-460
def valid_actions_from_state(state):
    """Devuelve las acciones vÃ¡lidas dado el estado (fase normal o defuse)."""
    phase_defuse = state[10] > 0.5
    if phase_defuse:
        return [3, 4, 5]  # Solo posiciones de Defuse
    else:
        return [0, 1, 2, 6, 7, 8]  # Draw, Skip, Attack, SeeFuture, DrawBottom, Shuffle
```

**Pero esto NO verifica si el agente tiene las cartas en mano**, solo filtra por fase.

---

## ğŸ¯ **Respuesta Directa a Tu Pregunta**

> **Â¿Por quÃ© la polÃ­tica puede elegir esa carta si no la tiene?**

Porque la red neuronal **NO recibe informaciÃ³n sobre SeeFuture, DrawBottom, Shuffle, ni Safe** en el vector de estado.

La red solo sabe:
- âœ… CuÃ¡ntos Defuse tiene (posiciÃ³n 3 del estado)
- âœ… CuÃ¡ntos Skip tiene (posiciÃ³n 4)
- âœ… CuÃ¡ntos Attack tiene (posiciÃ³n 5)
- âŒ **NO sabe** cuÃ¡ntos SeeFuture tiene
- âŒ **NO sabe** cuÃ¡ntos DrawBottom tiene
- âŒ **NO sabe** cuÃ¡ntos Shuffle tiene

Por eso puede **intentar** usar SeeFuture incluso sin tenerlo, porque **no tiene esa informaciÃ³n en la entrada**.

---

## ğŸ“ˆ **Impacto en el Entrenamiento**

### **Durante el entrenamiento:**
- La red aprende que elegir acciÃ³n 6 (SeeFuture) cuando **no produce cambio** de estado â†’ **reward bajo**
- Con el tiempo, aprende a elegir acciÃ³n 6 solo cuando **histÃ³ricamente ha funcionado**
- Esto es **aprendizaje implÃ­cito**: la red no sabe explÃ­citamente si tiene la carta, pero aprende patrones de cuÃ¡ndo es Ãºtil intentarlo

### **Resultado:**
- El agente desarrolla una "intuiciÃ³n" de cuÃ¡ndo tiene SeeFuture basÃ¡ndose en:
  - Â¿RobÃ³ una carta recientemente?
  - Â¿QuÃ© pasÃ³ la Ãºltima vez que intentÃ³ SeeFuture?
  - Â¿CuÃ¡ntas cartas tiene el oponente?

---

## ğŸ” **VerificaciÃ³n en el CÃ³digo**

Puedes ver exactamente quÃ© se incluye en el estado buscando en:
- **Archivo**: `v2/exploding_env.py`
- **FunciÃ³n**: `_get_obs()` (lÃ­neas 187-217)
- **Resultado**: Array de 11 valores, donde **NO** aparecen SeeFuture, DrawBottom, Shuffle

---

## ğŸ’¬ **ConclusiÃ³n**

La polÃ­tica DQN **no es omnisciente**. Solo ve:
1. Estado del deck (tamaÃ±o, bombas, probabilidad)
2. **Algunas** de sus cartas (Defuse, Skip, Attack)
3. InformaciÃ³n del oponente (total de cartas, Ãºltima acciÃ³n)

Y debe **aprender por experiencia** cuÃ¡ndo tiene otras cartas (SeeFuture, etc.) basÃ¡ndose en patrones indirectos.

Por eso:
- âœ… **Puede intentar** cualquier acciÃ³n (0-8)
- âŒ **No siempre se ejecuta** (si no tiene la carta)
- ğŸ“Š **CSV muestra ambos**: intentos y ejecuciones

---

**Â¿Te queda claro ahora por quÃ© SeeFuture aparece tanto en los intentos pero no siempre en las ejecuciones?**
